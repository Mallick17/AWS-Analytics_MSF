#!/usr/bin/env python3
# ==================================================================================
# MODULAR PYFLINK STREAMING JOB - COMPLETE WORKING VERSION
# ==================================================================================

import sys
import os
import yaml
import time

# FORCE ALL OUTPUT TO CLOUDWATCH
sys.stdout = sys.stderr

print("=" * 100)
print("STARTING - MODULAR PYFLINK STREAMING JOB")
print("Time: " + time.strftime("%Y-%m-%d %H:%M:%S"))
print("=" * 100)

# ================================================================================
# CRITICAL: Get the script's directory, not the current working directory
# ================================================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
print(f"\nScript Directory: {SCRIPT_DIR}")
print(f"Current Working Directory: {os.getcwd()}")

# Add script directory to Python path so modules can be imported
if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)
    print(f"Added {SCRIPT_DIR} to sys.path")

print("-" * 100)

# ================================================================================
# List files from SCRIPT_DIR (where your files actually are)
# ================================================================================
print("\nDirectory Structure and Files (from SCRIPT_DIR):")
print("-" * 100)

for root, dirs, files in os.walk(SCRIPT_DIR):
    level = root.replace(SCRIPT_DIR, '').count(os.sep)
    indent = ' ' * 2 * level
    print(f'{indent}{os.path.basename(root)}/')
    
    sub_indent = ' ' * 2 * (level + 1)
    for file in files:
        file_path = os.path.join(root, file)
        try:
            file_size = os.path.getsize(file_path)
            print(f'{sub_indent}{file} ({file_size} bytes)')
        except:
            print(f'{sub_indent}{file}')

print("=" * 100)

# ================================================================================
# PHASE 1: ENVIRONMENT CHECKS - USING SCRIPT_DIR
# ================================================================================
print("\n[PHASE 1] CHECKING ENVIRONMENT")
print(f"  Python running in: {os.getcwd()}")
print(f"  Script located in: {SCRIPT_DIR}")
print(f"  Python version: {sys.version}")

# Use SCRIPT_DIR for all file paths
required_files = [
    ("config/topics.yaml", os.path.join(SCRIPT_DIR, "config", "topics.yaml")),
    ("lib/pyflink-dependencies.jar", os.path.join(SCRIPT_DIR, "lib", "pyflink-dependencies.jar"))
]

print("\n  CHECKING REQUIRED FILES:")
all_files_exist = True

for display_name, file_path in required_files:
    exists = os.path.exists(file_path)
    size = os.path.getsize(file_path) if exists else 0
    print(f"    {display_name}: {'OK' if exists else 'MISSING'} ({size} bytes)")
    print(f"      Path: {file_path}")
    
    if not exists:
        all_files_exist = False
        if display_name == "config/topics.yaml":
            print("      ERROR: topics.yaml missing - cannot continue")
        elif display_name == "lib/pyflink-dependencies.jar":
            print("      ERROR: pyflink-dependencies.jar missing - cannot inject dependencies")

if not all_files_exist:
    print("\nFATAL: Required files missing!")
    sys.exit(1)

print("[PHASE 1] COMPLETE")

# ================================================================================
# PHASE 2: PYFLINK IMPORTS
# ================================================================================
print("\n[PHASE 2] PYFLINK IMPORTS")
try:
    print("  Importing pyflink.table...")
    from pyflink.table import EnvironmentSettings, TableEnvironment
    print("  pyflink.table OK")
    
    print("  Importing pyflink.java_gateway...")
    from pyflink.java_gateway import get_gateway
    print("  pyflink.java_gateway OK")
    
except ImportError as e:
    print(f"FATAL: PYFLINK IMPORT FAILED: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

print("[PHASE 2] COMPLETE")

# ================================================================================
# PHASE 3: JAR INJECTION - USING SCRIPT_DIR
# ================================================================================
print("\n[PHASE 3] JAR INJECTION")
try:
    gateway = get_gateway()
    jvm = gateway.jvm
    print("  JVM gateway OK")
    
    jar_path = os.path.join(SCRIPT_DIR, "lib", "pyflink-dependencies.jar")
    print(f"  JAR path: {jar_path}")
    
    if os.path.exists(jar_path):
        jar_url = jvm.java.net.URL(f"file://{jar_path}")
        jvm.Thread.currentThread().getContextClassLoader().addURL(jar_url)
        print("  JAR injected successfully")
    else:
        print("FATAL: Dependency JAR missing - cannot continue")
        sys.exit(1)
        
except Exception as e:
    print(f"FATAL: JAR INJECTION FAILED: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

print("[PHASE 3] COMPLETE")

# ================================================================================
# PHASE 4: LOAD CONFIGURATION
# ================================================================================
print("\n[PHASE 4] LOADING CONFIGURATION")

try:
    from common.config import Config, FLINK_CONFIG, ICEBERG_CONFIG
    print("  ‚úì Config module imported")
    
    # Initialize configuration loader
    config_dir = os.path.join(SCRIPT_DIR, "config")
    print(f"  Config directory: {config_dir}")
    
    config = Config(config_dir=config_dir)
    print("  ‚úì Configuration loaded successfully")
    
    # Get Kafka configuration
    kafka_config = config.get_kafka_config()
    print(f"  ‚úì Kafka bootstrap servers: {kafka_config.get('bootstrap_servers', 'MISSING')[:50]}...")
    
    # Get enabled topics
    enabled_topics = config.get_enabled_topics()
    print(f"  ‚úì Enabled topics: {enabled_topics}")
    
    if not enabled_topics:
        print("  WARNING: No enabled topics found!")
    
except Exception as e:
    print(f"FATAL: CONFIGURATION LOAD FAILED: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

print("[PHASE 4] COMPLETE")

# ================================================================================
# PHASE 5: FLINK TABLE ENVIRONMENT
# ================================================================================
print("\n[PHASE 5] FLINK TABLE ENVIRONMENT")

try:
    env_settings = EnvironmentSettings.in_streaming_mode()
    table_env = TableEnvironment.create(env_settings)
    print("  ‚úì TableEnvironment created")
    
    # Apply Flink configuration
    flink_config = table_env.get_config().get_configuration()
    flink_config.set_string("table.exec.resource.default-parallelism", FLINK_CONFIG.get("parallelism", "1"))
    flink_config.set_string("execution.checkpointing.interval", FLINK_CONFIG.get("checkpointing_interval", "60s"))
    flink_config.set_string("execution.checkpointing.mode", FLINK_CONFIG.get("checkpointing_mode", "EXACTLY_ONCE"))
    print("  ‚úì Flink configuration applied:")
    print(f"    - Parallelism: {FLINK_CONFIG.get('parallelism')}")
    print(f"    - Checkpointing interval: {FLINK_CONFIG.get('checkpointing_interval')}")
    print(f"    - Checkpointing mode: {FLINK_CONFIG.get('checkpointing_mode')}")
    
except Exception as e:
    print(f"FATAL: FLINK ENV FAILED: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

print("[PHASE 5] COMPLETE")

# ================================================================================
# PHASE 6: S3 TABLES CATALOG
# ================================================================================
print("\n[PHASE 6] S3 TABLES CATALOG")

try:
    from common.catalog_manager import CatalogManager
    print("  ‚úì CatalogManager imported")
except ImportError as e:
    print(f"FATAL: Cannot import CatalogManager: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

# Use ICEBERG_CONFIG from config.py (with environment variable overrides)
iceberg_config = {
    "warehouse": os.getenv("S3_WAREHOUSE", ICEBERG_CONFIG["warehouse"]),
    "region": os.getenv("AWS_REGION", ICEBERG_CONFIG["region"]),
    "namespace": os.getenv("ICEBERG_NAMESPACE", ICEBERG_CONFIG["namespace"])
}

print(f"  Iceberg Configuration:")
print(f"    Warehouse: {iceberg_config['warehouse']}")
print(f"    Region: {iceberg_config['region']}")
print(f"    Namespace: {iceberg_config['namespace']}")

# Create and initialize catalog using CatalogManager
try:
    catalog_manager = CatalogManager(table_env, iceberg_config)
    catalog_manager.create_catalog()
except Exception as e:
    print(f"FATAL: S3 TABLES CATALOG SETUP FAILED: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

print("[PHASE 6] COMPLETE")

# ================================================================================
# PHASE 7: PROCESS ENABLED TOPICS - CREATE PIPELINES
# ================================================================================
print("\n[PHASE 7] PROCESSING TOPICS")

# Import modular components
try:
    from common.kafka_sources import KafkaSourceCreator
    from common.iceberg_sinks import IcebergSinkCreator
    from common.utils import load_and_instantiate_transformer, validate_topic_config
    print("  ‚úì Modular components imported")
except ImportError as e:
    print(f"FATAL: Cannot import modular components: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

# Load transformations registry
try:
    transformations_registry = config.get_transformations_config()
    print(f"  ‚úì Loaded {len(transformations_registry)} transformation(s):")
    for trans_name in transformations_registry:
        trans_config = transformations_registry[trans_name]
        print(f"    ‚Ä¢ {trans_name}")
        print(f"        Class: {trans_config.get('class', 'MISSING')}")
        print(f"        Module: {trans_config.get('module', 'MISSING')}")
except Exception as e:
    print(f"FATAL: Cannot load transformations registry: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

# Get catalog and namespace from catalog_manager
catalog_name = catalog_manager.get_catalog_name()
namespace = catalog_manager.get_namespace()

# Get Kafka global config from Config object
kafka_global = config.get_kafka_config()

# Initialize modular creators (reusable across all topics)
print("\n  Initializing modular components:")
kafka_creator = KafkaSourceCreator(table_env, kafka_global)
print("    ‚úì KafkaSourceCreator initialized")

iceberg_creator = IcebergSinkCreator(table_env, catalog_manager, ICEBERG_CONFIG)
print("    ‚úì IcebergSinkCreator initialized")

# ================================================================================
# ‚≠ê CRITICAL CHANGE: Create StatementSet for concurrent job execution
# ================================================================================
print("\n  ‚≠ê Creating StatementSet for concurrent pipeline execution")
stmt_set = table_env.create_statement_set()
print("    ‚úì StatementSet created")

# Track pipelines
pipelines_created = 0
pipeline_info = []  # Store pipeline information for logging

# Process each enabled topic
for topic_name in enabled_topics:
    print(f"\n  {'=' * 80}")
    print(f"  PROCESSING TOPIC: {topic_name}")
    print(f"  {'=' * 80}")
    
    try:
        # Get topic-specific configuration
        topic_config = config.get_topic_config(topic_name)
        
        # Validate topic configuration has all required fields
        validate_topic_config(topic_config, topic_name)
        
        # ========================================================================
        # STEP 1: Create Kafka Source Table (using KafkaSourceCreator)
        # ========================================================================
        print("\n  [STEP 1] Creating Kafka Source")
        kafka_table = kafka_creator.create_source(topic_name, topic_config)
        
        # ========================================================================
        # STEP 2: Create Iceberg Sink Table (using IcebergSinkCreator)
        # ========================================================================
        print("\n  [STEP 2] Creating Iceberg Sink")
        sink_table = iceberg_creator.create_sink(topic_config)
        
        # ========================================================================
        # STEP 3: Load and Execute Transformer (using modular transformer classes)
        # ========================================================================
        print("\n  [STEP 3] Loading Transformation")
        
        # Get transformation name from topic config
        transformation_name = topic_config.get('transformation')
        if not transformation_name:
            raise ValueError(
                f"No transformation specified for topic '{topic_name}'. "
                f"Add 'transformation: <name>' to topic config in topics.yaml"
            )
        
        # Load and instantiate transformer using utils
        transformer = load_and_instantiate_transformer(
            transformation_name,
            transformations_registry,
            topic_config
        )
        
        # Get transformation SQL from transformer
        # Provide fully qualified source table name
        source_table_fqn = f"`default_catalog`.`default_database`.`{kafka_table}`"
        
        print(f"    Generating transformation SQL...")
        transformation_sql = transformer.get_transformation_sql(source_table_fqn)
        print(f"    ‚úì Transformation SQL generated")
        
        # Optional: Print SQL preview for debugging (first 10 lines)
        sql_lines = transformation_sql.strip().split('\n')
        if len(sql_lines) <= 10:
            print(f"    SQL Preview:")
            for line in sql_lines:
                print(f"      {line.strip()}")
        else:
            print(f"    SQL Preview (first 10 lines):")
            for line in sql_lines[:10]:
                print(f"      {line.strip()}")
            print(f"      ... ({len(sql_lines) - 10} more lines)")
        
        # ========================================================================
        # STEP 4: Add INSERT Statement to StatementSet (NOT execute immediately)
        # ========================================================================
        print("\n  [STEP 4] Adding Pipeline to StatementSet")
        
        # Build INSERT statement using transformer's SQL
        insert_sql = f"""
            INSERT INTO `{catalog_name}`.`{namespace}`.`{sink_table}`
            {transformation_sql}
        """
        
        print(f"    INSERT INTO: {catalog_name}.{namespace}.{sink_table}")
        print(f"    FROM: {kafka_table}")
        print(f"    Adding to statement set...")
        
        # ‚≠ê KEY CHANGE: Add to StatementSet instead of executing immediately
        stmt_set.add_insert_sql(insert_sql)
        
        pipelines_created += 1
        pipeline_info.append({
            'topic': topic_name,
            'source': kafka_table,
            'sink': sink_table,
            'transformation': transformation_name
        })
        
        print(f"\n    ‚úì ‚úì ‚úì Pipeline ADDED to StatementSet: {topic_name} ‚úì ‚úì ‚úì")
        
    except ValueError as e:
        # Configuration or validation errors
        print(f"\n    ‚úó CONFIGURATION ERROR for topic {topic_name}:", file=sys.stderr)
        print(f"    {e}", file=sys.stderr)
        print(f"    Skipping this topic and continuing...\n")
        continue
        
    except ImportError as e:
        # Transformer loading errors
        print(f"\n    ‚úó TRANSFORMER LOAD ERROR for topic {topic_name}:", file=sys.stderr)
        print(f"    {e}", file=sys.stderr)
        print(f"    Skipping this topic and continuing...\n")
        continue
        
    except Exception as e:
        # Any other errors
        print(f"\n    ‚úó ERROR processing topic {topic_name}:", file=sys.stderr)
        print(f"    {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        print(f"    Skipping this topic and continuing...\n")
        continue

print(f"\n{'=' * 100}")
print(f"[PHASE 7] COMPLETE - {pipelines_created} pipeline(s) added to StatementSet")
print(f"{'=' * 100}")

# ================================================================================
# PHASE 8: EXECUTE ALL PIPELINES CONCURRENTLY
# ================================================================================
print("\n[PHASE 8] EXECUTING ALL PIPELINES")

if pipelines_created == 0:
    print("  ‚úó WARNING: No pipelines were created!")
    print("  Check topic configurations:")
    print("    1. Ensure at least one topic has 'enabled: true'")
    print("    2. Verify source and sink schemas are correct")
    print("    3. Check logs above for errors")
    sys.exit(1)

# Display pipeline summary
print(f"  ‚úì {pipelines_created} pipeline(s) ready for execution")
print("\n  Pipeline Summary:")
for idx, info in enumerate(pipeline_info, 1):
    print(f"    {idx}. Topic: {info['topic']}")
    print(f"       Source: {info['source']}")
    print(f"       Sink: {info['sink']}")
    print(f"       Transformation: {info['transformation']}")
    print()

# ‚≠ê CRITICAL: Execute all pipelines together
print("  üöÄ Executing StatementSet (all pipelines will run concurrently)...")
print("-" * 100)

try:
    # Execute the statement set - this creates separate Flink jobs for each INSERT
    job_result = stmt_set.execute()
    
    print(f"\n  ‚úì ‚úì ‚úì ALL PIPELINES STARTED SUCCESSFULLY ‚úì ‚úì ‚úì")
    print(f"\n  Expected Flink Jobs: {pipelines_created}")
    print("  Each pipeline runs as a separate streaming job")
    print("\n  Job Lifecycle:")
    print("    ‚Ä¢ Pipelines will process data continuously")
    print("    ‚Ä¢ AWS Managed Flink handles failure recovery")
    print("    ‚Ä¢ Data flows: Kafka ‚Üí Flink ‚Üí S3 Tables (Iceberg)")
    print("\n  To view jobs: Check Flink Dashboard in AWS Console")
    print("  To stop: Stop the Kinesis Data Analytics application")
    print("-" * 100)
    
    print("\n  Entering streaming mode - jobs will run indefinitely...")
    
    # Wait for job completion (streaming jobs run forever)
    job_result.wait()
    
except KeyboardInterrupt:
    print("\n  Job interrupted by user")
except Exception as e:
    print(f"\n  Pipeline execution error: {e}")
    import traceback
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

print("\n" + "=" * 100)
print("MODULAR PYFLINK JOB EXECUTION COMPLETE")
print("Time: " + time.strftime("%Y-%m-%d %H:%M:%S"))
print("=" * 100)