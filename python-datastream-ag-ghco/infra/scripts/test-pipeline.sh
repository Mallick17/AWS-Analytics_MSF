#!/bin/bash
# ==================================================================================
# COMPREHENSIVE INTEGRATION TEST SCRIPT
# ==================================================================================
# Tests the complete pipeline: Docker setup ‚Üí Job execution ‚Üí Data verification
# ==================================================================================

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Get script directory and navigate to project root
SCRIPT_DIR=$(dirname "$0")
PROJECT_ROOT=$(cd "$SCRIPT_DIR/../.." && pwd)

echo -e "${BLUE}=========================================="
echo "COMPREHENSIVE PIPELINE TEST"
echo "=========================================="
echo -e "Project Root: $PROJECT_ROOT${NC}"
echo "Time: $(date)"
echo

# Step 1: Verify configuration
echo -e "${YELLOW}üìã Step 1: Verifying Configuration...${NC}"

if [ ! -f "$PROJECT_ROOT/.env" ]; then
    echo -e "${RED}‚ùå Error: .env file not found!${NC}"
    echo "   Please copy .env.example to .env and configure it:"
    echo "   cp .env.example .env"
    exit 1
fi

# Source the environment file
source "$PROJECT_ROOT/.env"

# Check critical variables
if [ -z "$S3_WAREHOUSE" ]; then
    echo -e "${RED}‚ùå Error: S3_WAREHOUSE not set in .env${NC}"
    exit 1
fi

if [[ ! "$S3_WAREHOUSE" == arn:aws:s3tables:* ]]; then
    echo -e "${RED}‚ùå Error: S3_WAREHOUSE must be an S3 Tables ARN format${NC}"
    echo "   Current value: $S3_WAREHOUSE"
    echo "   Expected format: arn:aws:s3tables:<region>:<account>:bucket/<bucket-name>"
    exit 1
fi

if [ -z "$S3_WAREHOUSE_BUCKET" ]; then
    echo -e "${RED}‚ùå Error: S3_WAREHOUSE_BUCKET not set in .env${NC}"
    exit 1
fi

echo -e "${GREEN}‚úì Configuration verified${NC}"
echo "  S3 Tables Warehouse: $S3_WAREHOUSE"
echo "  Standard S3 Bucket: $S3_WAREHOUSE_BUCKET"
echo "  Flink Environment: $FLINK_ENV"

# Step 2: Start infrastructure
echo -e "\n${YELLOW}üê≥ Step 2: Starting Infrastructure...${NC}"

cd "$PROJECT_ROOT"

# Stop any existing containers
docker-compose -f infra/docker-compose.yml down --remove-orphans >/dev/null 2>&1 || true

# Start containers
if ! docker-compose -f infra/docker-compose.yml up -d; then
    echo -e "${RED}‚ùå Failed to start Docker containers${NC}"
    exit 1
fi

echo -e "${GREEN}‚úì Docker containers started${NC}"

# Step 3: Wait for services to be ready
echo -e "\n${YELLOW}‚è≥ Step 3: Waiting for Services...${NC}"

# Wait for Kafka
echo "  Waiting for Kafka..."
for i in {1..60}; do
    if docker-compose -f infra/docker-compose.yml exec -T kafka kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1; then
        echo -e "${GREEN}  ‚úì Kafka is ready${NC}"
        break
    fi
    if [ $i -eq 60 ]; then
        echo -e "${RED}  ‚ùå Timeout: Kafka not ready${NC}"
        exit 1
    fi
    sleep 2
done

# Wait for Flink JobManager
echo "  Waiting for Flink JobManager..."
for i in {1..30}; do
    if docker-compose -f infra/docker-compose.yml exec -T jobmanager curl -s http://localhost:8081/overview >/dev/null 2>&1; then
        echo -e "${GREEN}  ‚úì Flink JobManager is ready${NC}"
        break
    fi
    if [ $i -eq 30 ]; then
        echo -e "${RED}  ‚ùå Timeout: Flink JobManager not ready${NC}"
        exit 1
    fi
    sleep 2
done

# Wait for Iceberg REST Catalog
echo "  Waiting for Iceberg REST Catalog..."
for i in {1..20}; do
    if curl -s http://localhost:8181/v1/config >/dev/null 2>&1; then
        echo -e "${GREEN}  ‚úì Iceberg REST Catalog is ready${NC}"
        break
    fi
    if [ $i -eq 20 ]; then
        echo -e "${RED}  ‚ùå Timeout: Iceberg REST Catalog not ready${NC}"
        exit 1
    fi
    sleep 2
done

# Step 4: Initialize Kafka topics
echo -e "\n${YELLOW}üìù Step 4: Initializing Kafka Topics...${NC}"

if ! "$PROJECT_ROOT/infra/scripts/setup_local.sh"; then
    echo -e "${RED}‚ùå Failed to setup Kafka topics${NC}"
    exit 1
fi

echo -e "${GREEN}‚úì Kafka topics initialized${NC}"

# Step 5: Start the Flink job in background
echo -e "\n${YELLOW}üöÄ Step 5: Starting Flink Job...${NC}"

# Start job in background and capture output
JOB_LOG="/tmp/flink_job_test.log"
docker-compose -f infra/docker-compose.yml exec -T jobmanager \
    bash -c "cd /opt/flink/usrlib && timeout 30s python streaming_job.py || true" > "$JOB_LOG" 2>&1 &

JOB_PID=$!

# Wait a bit for job to initialize
sleep 10

# Check if job started successfully
if ps -p $JOB_PID > /dev/null; then
    echo -e "${GREEN}‚úì Flink job started successfully${NC}"
else
    echo -e "${RED}‚ùå Flink job failed to start${NC}"
    echo "Job output:"
    cat "$JOB_LOG"
    exit 1
fi

# Step 6: Test with sample data
echo -e "\n${YELLOW}üìä Step 6: Testing with Sample Data...${NC}"

# Generate some test data
if [ -f "$PROJECT_ROOT/sample_gen.py" ]; then
    echo "  Generating sample data..."
    timeout 10s python "$PROJECT_ROOT/sample_gen.py" >/dev/null 2>&1 || true
    echo -e "${GREEN}  ‚úì Sample data generated${NC}"
else
    echo -e "${YELLOW}  ‚ö†Ô∏è  sample_gen.py not found, skipping data generation${NC}"
fi

# Step 7: Verify job status
echo -e "\n${YELLOW}üîç Step 7: Verifying Job Status...${NC}"

sleep 5  # Give time for processing

# Check Flink dashboard for running jobs
if docker-compose -f infra/docker-compose.yml exec -T jobmanager \
   curl -s http://localhost:8081/jobs | grep -q "RUNNING"; then
    echo -e "${GREEN}‚úì Jobs are running on Flink cluster${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  No running jobs detected (may be normal for quick test)${NC}"
fi

# Clean up the background job
kill $JOB_PID >/dev/null 2>&1 || true

# Step 8: Results and next steps
echo -e "\n${BLUE}=========================================="
echo "INTEGRATION TEST COMPLETE"
echo "=========================================="
echo -e "Time: $(date)${NC}"
echo
echo -e "${GREEN}‚úÖ SUCCESS: Pipeline infrastructure is working correctly${NC}"
echo
echo "üìä Monitor the pipeline:"
echo "  ‚Ä¢ Flink Dashboard: http://localhost:8081"
echo "  ‚Ä¢ View logs: docker-compose -f infra/docker-compose.yml logs -f jobmanager"
echo
echo "üöÄ Run the full pipeline:"
echo "  ‚Ä¢ Foreground: ./infra/scripts/run-job.sh"
echo "  ‚Ä¢ Background: ./infra/scripts/run-job-background.sh"
echo
echo "üõë Stop the pipeline:"
echo "  ‚Ä¢ Run: ./infra/scripts/stop.sh"
echo
echo "üìù Production deployment:"
echo "  ‚Ä¢ Build: mvn clean package"
echo "  ‚Ä¢ Deploy: Upload target/pyflink-s3tables-app.zip to AWS Managed Flink"
echo
if [ "$FLINK_ENV" = "production" ]; then
    echo -e "${BLUE}üí° NOTE: Running in PRODUCTION mode - data will be written to S3 Tables${NC}"
    echo "   S3 Tables ARN: $S3_WAREHOUSE"
else
    echo -e "${BLUE}üí° NOTE: Running in LOCAL mode - data will be printed to console${NC}"
    echo "   Set FLINK_ENV=production in .env for S3 Tables output"
fi
