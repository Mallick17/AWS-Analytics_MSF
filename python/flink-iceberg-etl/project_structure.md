# Project Structure - Simplified

## ğŸ“ Files You Need

```
flink-iceberg-etl/
â”‚
â”œâ”€â”€ main.py                      âœ… MAIN APPLICATION (use this)
â”‚   â””â”€ PyIceberg + S3 Tables
â”‚   â””â”€ Kafka â†’ Transform â†’ S3 Tables
â”‚
â”œâ”€â”€ requirements.txt             âœ… Python dependencies
â”‚   â””â”€ pyflink, pyiceberg, boto3, etc.
â”‚
â”œâ”€â”€ .pyiceberg.yaml             âœ… S3 Tables catalog config
â”‚   â””â”€ Warehouse ARN, region, etc.
â”‚
â”œâ”€â”€ test_local.py               âœ… Local testing
â”‚   â””â”€ Test without Flink cluster
â”‚
â”œâ”€â”€ build.sh                    âœ… Build & package script
â”‚   â””â”€ Downloads JARs, creates ZIP
â”‚
â”œâ”€â”€ DEPLOYMENT.md               âœ… Deployment guide
â”‚   â””â”€ Step-by-step AWS setup
â”‚
â”œâ”€â”€ README.md                   âœ… Project documentation
â”‚   â””â”€ Quick start, architecture
â”‚
â””â”€â”€ S3_TABLES_EXPLAINED.md      âœ… Understanding S3 Tables
    â””â”€ What is S3 Tables vs S3
```

## âŒ Files Removed

```
main_native_iceberg.py          âŒ REMOVED (not needed)
â””â”€ Why removed:
   â€¢ Flink native connector doesn't fully support S3 Tables
   â€¢ PyIceberg is AWS's recommended approach
   â€¢ Adds complexity without benefits
   â€¢ Would require additional JAR dependencies
```

## ğŸ¯ Why Only One Main File?

### main.py (What We Use) âœ…

```python
# Uses PyIceberg library
from pyiceberg.catalog import load_catalog

# Direct S3 Tables support
catalog = load_catalog(
    "s3_tables",
    **{
        "type": "glue",
        "warehouse": "arn:aws:s3tables:...",  # S3 Tables ARN
    }
)

# Simple, direct writes
table.append(data)  # S3 Tables handles everything!
```

**Benefits:**
- âœ… Direct S3 Tables support
- âœ… No extra dependencies
- âœ… Simpler code
- âœ… AWS recommended
- âœ… Active development

### main_native_iceberg.py (Removed) âŒ

```python
# Would use Flink Table API + SQL
from pyflink.table import StreamTableEnvironment

# Create Iceberg catalog in Flink
table_env.execute_sql("""
    CREATE CATALOG iceberg_catalog WITH (
        'type' = 'iceberg',
        'catalog-impl' = 'software.amazon.s3tables.iceberg.S3TablesCatalog',
        ...
    )
""")
```

**Problems:**
- âŒ More complex setup
- âŒ S3 Tables support still maturing
- âŒ Requires additional JARs
- âŒ Less flexible
- âŒ Harder to debug

## ğŸ“¦ What Gets Deployed

When you run `build.sh`, it creates:

```
flink-iceberg-etl.zip
â”œâ”€â”€ main.py                 â† Your application
â”œâ”€â”€ requirements.txt        â† Dependencies list
â”œâ”€â”€ .pyiceberg.yaml        â† Catalog config
â””â”€â”€ lib/                   â† JAR dependencies
    â”œâ”€â”€ flink-sql-connector-kafka-1.18.0.jar
    â”œâ”€â”€ aws-msk-iam-auth-1.1.6-all.jar
    â”œâ”€â”€ iceberg-flink-runtime-1.18-1.4.3.jar
    â”œâ”€â”€ iceberg-aws-bundle-1.4.3.jar
    â””â”€â”€ bundle-2.20.18.jar
```

**Upload to:**
```
s3://testing-python-flink-connector/applications/flink-iceberg-etl.zip
```

## ğŸ”„ Workflow

```
1. Development
   â”œâ”€â”€ Edit main.py
   â”œâ”€â”€ Test with test_local.py
   â””â”€â”€ Update requirements.txt

2. Build
   â”œâ”€â”€ Run build.sh
   â””â”€â”€ Creates flink-iceberg-etl.zip

3. Deploy
   â”œâ”€â”€ Upload to S3: testing-python-flink-connector
   â”œâ”€â”€ Create Flink app in AWS Console
   â””â”€â”€ Configure environment variables

4. Run
   â”œâ”€â”€ Start Flink application
   â”œâ”€â”€ Monitor CloudWatch logs
   â””â”€â”€ Query data in Athena
```

## ğŸ“Š Data Flow

```
Source                  Processing              Destination
â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MSK Kafka    â†’    PyFlink (main.py)    â†’    S3 Tables
user_events           â”‚                       sink.user_events
                      â”œâ”€ Parse JSON
                      â”œâ”€ Transform
                      â”œâ”€ Batch (100)
                      â””â”€ PyIceberg
                         â””â”€ table.append()
                            â””â”€ Auto-optimized!
```

## ğŸ“ File Purposes

### main.py - Core Application

**What it does:**
1. Connects to MSK Kafka with IAM auth
2. Reads messages from topics
3. Parses and transforms JSON
4. Batches records (100 at a time)
5. Writes to S3 Tables using PyIceberg
6. S3 Tables auto-optimizes everything

**Key classes:**
- `KafkaToS3TablesETL` - Main ETL orchestrator
- `S3TablesWriter` - Batch writer for S3 Tables
- Uses PyIceberg for all Iceberg operations

### requirements.txt - Dependencies

```txt
apache-flink==1.18.1        # PyFlink framework
pyflink==1.18.1             # Python Flink API
pyiceberg[pyarrow,s3fs,glue]==0.6.1  # S3 Tables support
boto3>=1.34.0               # AWS SDK
pyarrow>=14.0.0             # Arrow format
```

### .pyiceberg.yaml - Catalog Config

```yaml
catalog:
  s3_tables:
    type: glue                # S3 Tables uses Glue
    warehouse: arn:aws:s3tables:...  # Your S3 Tables bucket
    client.region: ap-south-1
    io-impl: org.apache.iceberg.aws.s3.S3FileIO
```

### test_local.py - Local Testing

**Tests:**
- âœ… Message transformation
- âœ… Schema validation
- âœ… Iceberg operations (local)
- âœ… S3 Tables connection (if credentials available)
- âœ… Performance benchmarks

### build.sh - Build Automation

**What it does:**
1. Creates `build/` directory
2. Copies Python files
3. Downloads JAR dependencies
4. Creates `flink-iceberg-etl.zip`
5. Shows upload command

### DEPLOYMENT.md - Deployment Steps

**Covers:**
- IAM role setup
- S3 upload
- Flink app creation
- Environment variables
- Monitoring setup

### README.md - Quick Start

**Provides:**
- Architecture overview
- Quick start guide
- Configuration examples
- Troubleshooting tips

### S3_TABLES_EXPLAINED.md - Concepts

**Explains:**
- S3 Tables vs Regular S3
- Why use PyIceberg
- ARN format differences
- Data flow

## ğŸ¯ Decision Tree

```
Need to modify code?
â”œâ”€ Yes â†’ Edit main.py
â””â”€ No
   â”‚
   Need to add dependency?
   â”œâ”€ Yes â†’ Update requirements.txt
   â””â”€ No
      â”‚
      Need to test locally?
      â”œâ”€ Yes â†’ Run test_local.py
      â””â”€ No
         â”‚
         Ready to deploy?
         â”œâ”€ Yes â†’ Run build.sh â†’ Upload ZIP
         â””â”€ No â†’ Read DEPLOYMENT.md
```

## âœ… Simplified Benefits

**Before (with both files):**
```
main.py + main_native_iceberg.py
â”œâ”€ Confusion: Which one to use?
â”œâ”€ Duplication: Similar functionality
â””â”€ Complexity: Two approaches
```

**After (single file):**
```
main.py only
â”œâ”€ Clear: One way to do it
â”œâ”€ Simple: PyIceberg + S3 Tables
â””â”€ Reliable: AWS recommended approach
```

## ğŸš€ Getting Started

```bash
# 1. Test locally
python test_local.py

# 2. Build package
./build.sh

# 3. Upload to S3
aws s3 cp flink-iceberg-etl.zip \
  s3://testing-python-flink-connector/applications/ \
  --region ap-south-1

# 4. Deploy in AWS Console
# Follow DEPLOYMENT.md

# 5. Start application
aws kinesisanalyticsv2 start-application \
  --application-name flink-iceberg-etl \
  --region ap-south-1
```

## ğŸ“ Summary

**Single file approach (`main.py`):**
- âœ… Uses PyIceberg (AWS recommended)
- âœ… Direct S3 Tables support
- âœ… Simpler architecture
- âœ… Easier to maintain
- âœ… Better for production

**No need for multiple approaches!**